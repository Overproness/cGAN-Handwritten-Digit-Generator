import streamlit as st
import torch
import torch.nn as nn
import numpy as np
import torchvision.utils as vutils

# --- 1. Model Architecture (Corrected to match the training script) ---

# Parameters (must match the training script)
LATENT_DIM = 100
N_CLASSES = 10
EMBEDDING_DIM = 50

# --- THIS IS THE CORRECT GENERATOR CLASS FROM YOUR TRAINING SCRIPT ---
# The original class in your app.py was a simple linear model, while your
# trained weights belong to this convolutional (DCGAN) model. This mismatch
# was the cause of the error.
class Generator(nn.Module):
    """
    Conditional DCGAN Generator.
    """
    def __init__(self):
        super(Generator, self).__init__()
        self.label_embedding = nn.Embedding(N_CLASSES, EMBEDDING_DIM)
        self.latent_dim = LATENT_DIM

        self.init_size = 7 # We will start with a 7x7 activation and upsample
        self.l1 = nn.Sequential(nn.Linear(self.latent_dim + EMBEDDING_DIM, 128 * self.init_size ** 2))

        self.conv_blocks = nn.Sequential(
            nn.BatchNorm2d(128),
            nn.Upsample(scale_factor=2), # Upsample to 14x14
            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128, 0.8),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Upsample(scale_factor=2), # Upsample to 28x28
            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64, 0.8),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1),
            nn.Tanh(),
        )

    def forward(self, noise, labels):
        gen_input = torch.cat((self.label_embedding(labels), noise), -1)
        out = self.l1(gen_input)
        out = out.view(out.shape[0], 128, self.init_size, self.init_size)
        img = self.conv_blocks(out)
        return img

# --- 2. Load the Trained Model ---

# Use a cache to load the model only once
@st.cache_resource
def load_model():
    model = Generator()
    # Load the weights from the file. Use map_location for CPU execution.
    model.load_state_dict(torch.load('cgan_generator.pth', map_location=torch.device('cpu')))
    model.eval() # Set the model to evaluation mode
    return model

generator = load_model()
device = torch.device("cpu") # App will run on CPU

# --- 3. Web Application UI ---

st.set_page_config(layout="wide")
st.title("Handwritten Digit Generation using a cGAN")

st.write("""
This web app generates images of handwritten digits (0-9). The images are generated by a Conditional Generative Adversarial Network (cGAN) trained on the MNIST dataset. Select a digit from the dropdown below and click 'Generate' to see five unique examples.
""")

st.divider()

# --- 4. User Input and Image Generation ---
col1, col2 = st.columns([1, 3])

with col1:
    # User selects which digit to generate
    selected_digit = st.selectbox("Select a digit to generate:", list(range(10)))

    # Generate button
    generate_button = st.button("Generate Images", type="primary")

with col2:
    if generate_button:
        # Generate 5 images
        num_images = 5

        # Prepare noise and labels
        with torch.no_grad(): # No need to track gradients
            noise = torch.randn(num_images, LATENT_DIM, device=device)
            labels = torch.full((num_images,), selected_digit, dtype=torch.long, device=device)

            # Generate images
            generated_images = generator(noise, labels)

            # Un-normalize the images from [-1, 1] to [0, 1] for display
            generated_images = generated_images * 0.5 + 0.5

            # Create a grid of images
            grid = vutils.make_grid(generated_images, nrow=5, padding=2, normalize=True)

            # Display the grid
            st.image(grid.permute(1, 2, 0).numpy(), caption=f"Generated Images for Digit: {selected_digit}")
    else:
        st.info("Select a digit and click 'Generate Images' to start.")